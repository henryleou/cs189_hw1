{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 189 HW1 Problems #1 - 6\n",
    "## By: Henry Leou 01/29/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "My Collaborators: Ryan Chan and Hung Ju Johnny Wang\n",
    "\n",
    "Disclaimer: I certify that all solutions are entirely in my own words and that I have not looked at another\n",
    "students solutions. I have given credit to all external sources I consulted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from scipy import io\n",
    "import random\n",
    "import sys\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import vstack\n",
    "import save_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Python Configuration and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HW1-1\n",
    "if sys.version_info[0] < 3:\n",
    "    raise Exception(\"Python 3 not detected.\")\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from scipy import io\n",
    "for data_name in [\"mnist\", \"spam\", \"cifar10\"]:\n",
    "    data = io.loadmat(\"data/%s_data.mat\" % data_name)\n",
    "    print(\"\\nloaded %s data!\" % data_name)\n",
    "    fields = \"test_data\", \"training_data\", \"training_labels\"\n",
    "    for field in fields:\n",
    "        print(field, data[field].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) In the beginning, I load the data set by doing io.loadmat. then I shuffled \n",
    "the data by using np.random.choice, and by giving it a seed using np.random.seed, in\n",
    "order to generate the same shuffling order for both of my data and labels. At last I\n",
    "subset the arrays by using slicing technique in order to get my training data sets, \n",
    "training label sets, validation data sets, and validation label sets.\n",
    "\n",
    "b) Similarity from part a), the same technique was done. Except we are using \n",
    "only 20% of the data, therefore, we mulitplied it by 0.2.\n",
    "\n",
    "c) This process is exactly the same as part a) except now we slice out 5000 features \n",
    "instead of 10000 which was done in part a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HW1 - 2a, b, and c\n",
    "#part a\n",
    "#load data\n",
    "np.random.seed(418)\n",
    "mnist = io.loadmat(\"data/mnist_data.mat\")\n",
    "num_samples = len(mnist[\"training_data\"])\n",
    "index_arr = np.random.choice(num_samples, num_samples, replace=False)\n",
    "\n",
    "mnist_validation_set_data = mnist[\"training_data\"][index_arr][:10000]\n",
    "mnist_validation_set_labels = mnist[\"training_labels\"][index_arr][:10000]\n",
    "mnist_training_set_data = mnist[\"training_data\"][index_arr][10000:]\n",
    "mnist_training_set_labels = mnist[\"training_labels\"][index_arr][10000:]\n",
    "print(\"validation for mnist data:{}\".format(mnist_validation_set_data.shape))\n",
    "print(\"validation for mnist labels:{}\".format(mnist_validation_set_labels.shape))\n",
    "print(\"training for mnist data:{}\".format(mnist_training_set_data.shape))\n",
    "print(\"training for mnist labels:{}\".format(mnist_training_set_labels.shape))\n",
    "#print(\"validation for mnist:{}\".format(mnist_set_labels.shape[0]))\n",
    "\n",
    "\n",
    "#part b)\n",
    "\n",
    "print()\n",
    "np.random.seed(419)\n",
    "#np.random.seed(0)\n",
    "spam = io.loadmat(\"data/spam_data.mat\")\n",
    "num_samples = len(spam[\"training_data\"])\n",
    "index_arr = np.random.choice(num_samples, num_samples, replace=False)\n",
    "\n",
    "spam_20 = int(spam[\"training_data\"][index_arr].shape[0] * 0.2)\n",
    "spam_validation_set_data = spam[\"training_data\"][index_arr][:spam_20]\n",
    "spam_validation_set_labels = spam[\"training_labels\"][index_arr][:spam_20]\n",
    "spam_training_set_data = spam[\"training_data\"][index_arr][spam_20:]\n",
    "spam_training_set_labels = spam[\"training_labels\"][index_arr][spam_20:]\n",
    "print(\"validation for spam data:{}\".format(spam_validation_set_data.shape))\n",
    "print(\"validation for spam labels:{}\".format(spam_validation_set_labels.shape))\n",
    "print(\"training for spam data:{}\".format(spam_training_set_data.shape))\n",
    "print(\"training for spam labels:{}\".format(spam_training_set_labels.shape))\n",
    "\n",
    "#part c)\n",
    "print()\n",
    "np.random.seed(420)\n",
    "cifar10 = io.loadmat(\"data/cifar10_data.mat\")\n",
    "num_samples = len(cifar10[\"training_data\"])\n",
    "index_arr = np.random.choice(num_samples, num_samples, replace=False)\n",
    "cifar10_validation_set_data = cifar10[\"training_data\"][index_arr][:5000]\n",
    "cifar10_validation_set_labels = cifar10[\"training_labels\"][index_arr][:5000]\n",
    "cifar10_training_set_data = cifar10[\"training_data\"][index_arr][5000:]\n",
    "cifar10_training_set_labels = cifar10[\"training_labels\"][index_arr][5000:]\n",
    "print(\"validation for cifar10 data:{}\".format(cifar10_validation_set_data.shape))\n",
    "print(\"validation for cifar10 labels:{}\".\n",
    "      format(cifar10_validation_set_labels.shape))\n",
    "print(\"training for cifar10 data:{}\".format(cifar10_training_set_data.shape))\n",
    "print(\"training for cifar10 labels:{}\".format\n",
    "      (cifar10_training_set_labels.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Support Vector Machines: Coding (part a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I created an array of all the sampling numbers that we want.\n",
    "Then I used a for loop for each of my sliced out training data,\n",
    "training label, validation data, and validation labels in order to\n",
    "generate my validation accuracy. At last in order to get my error rate,\n",
    "I simply did 1 - validation accuracy and append it to my total \n",
    "validation accuracy array in order to generate each points of\n",
    "data error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 3a\n",
    "#collect_train_acc = []\n",
    "collect_validation_acc = []\n",
    "trainings = [100, 200, 500, 1000, 2000, 5000, 10000]\n",
    "for i in trainings:\n",
    "    t_data = mnist_training_set_data[:i, :-1]\n",
    "    t_labels = mnist_training_set_labels[:i, -1:].ravel()\n",
    "    val_data = mnist_validation_set_data[:, :-1]\n",
    "    val_labels = mnist_validation_set_labels[:, -1:].ravel()\n",
    "    clf = svm.LinearSVC()\n",
    "    clf.fit(t_data, t_labels)\n",
    "    val_acc = clf.score(val_data, val_labels)\n",
    "\n",
    "    collect_validation_acc.append(1 - val_acc)\n",
    "    \n",
    "#plt.plot(trainings, collect_train_acc,\n",
    "#         label='Training data error', marker='x')  \n",
    "plt.plot(trainings, collect_validation_acc,\n",
    "         label='Validation data error', marker='x')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.legend(loc='upper right', frameon=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Support Vector Machines: Coding (part b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to part a) execpt we have a smaller training features, therefore,\n",
    "we get a higher error rate than part a. Also we can see that the line isn't\n",
    "as smooth as the one in part a) due to a smaller training features size.\n",
    "Generally, this is because with a small amount of data, we are more likely\n",
    "to get a prediction due by chance instead of actual. This would cause the\n",
    "data to become less reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#problem 3b\n",
    "#collect_train_acc = []\n",
    "collect_validation_acc = []\n",
    "trainings = [100, 200, 500, 1000, 2000, 4138]\n",
    "for i in trainings:\n",
    "    \n",
    "    # get train and valid data\n",
    "    t_data = spam_training_set_data[:i]\n",
    "    t_labels = spam_training_set_labels[:i].ravel()\n",
    "    val_data = spam_validation_set_data\n",
    "    val_labels = spam_validation_set_labels#.ravel()\n",
    "    \n",
    "    # train model\n",
    "    clf = svm.LinearSVC()\n",
    "    #clf = svm.SVC(kernel=\"linear\")\n",
    "    clf.fit(t_data, t_labels)\n",
    "    \n",
    "    # predict and cal accuracy\n",
    "    val_predict = clf.predict(val_data)\n",
    "    val_acc = clf.score(val_data, val_labels)\n",
    "\n",
    "    collect_validation_acc.append(1-val_acc)\n",
    "    print(val_acc)\n",
    "    \n",
    "#plt.plot(trainings, collect_train_acc,\n",
    "#          label='Training data accuracy', marker='x')  \n",
    "plt.plot(trainings, collect_validation_acc,\n",
    "         label='Validation data error', marker='x')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.legend(loc='upper right', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Support Vector Machines: Coding (part c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar process has been done in part a and b. However the SVM or Support Vector\n",
    "Machine hasn't been performing well on my predictions when we use natural\n",
    "gathered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 3c\n",
    "collect_train_acc = []\n",
    "collect_validation_acc = []\n",
    "trainings = [100, 200, 500, 1000, 2000, 5000]\n",
    "for i in trainings:\n",
    "    t_data = cifar10_training_set_data[:i, :-1]\n",
    "    t_labels = cifar10_training_set_labels[:i, -1:].ravel()\n",
    "    val_data = cifar10_validation_set_data[:, :-1]\n",
    "    val_labels = cifar10_validation_set_labels[:, -1:].ravel()\n",
    "    \n",
    "    clf = svm.SVC(kernel='linear')\n",
    "    clf.fit(t_data, t_labels)\n",
    "    val_acc = clf.score(val_data, val_labels)\n",
    "    collect_validation_acc.append(1-val_acc)\n",
    "\n",
    "plt.plot(trainings, collect_validation_acc,\n",
    "         label='Validation data error', marker='x')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.legend(loc='upper right', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first set up a possible range of values to test, then\n",
    "I limit the values by the trend from the output of my previous\n",
    "result based on the range of values that I set intitially.\n",
    "At last I found out that the optimal values should be between\n",
    "-6 and -4. Then I sliced my data and used LinearSVC to generate\n",
    "a list of C values and it's respective accuracy in which I found that\n",
    "1e-05 gives me the maximum accuracy of 0.9109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 4a\n",
    "\n",
    "collect_train_acc = []\n",
    "collect_validation_acc = []\n",
    "sample_size = 60000\n",
    "# c_values = [10**(i) for i in range(-10, 5, 1)] #list(range(1, 100))\n",
    "# found that the optimal values should be between -6 and -4\n",
    "c_values = [10**(j) for j in np.arange(-6, -4, 0.5)] \n",
    "val_data = mnist_validation_set_data[:, :-1]\n",
    "val_labels = mnist_validation_set_labels[:, -1:].ravel()\n",
    "train_data = mnist_training_set_data[:sample_size, :-1]\n",
    "train_labels = mnist_training_set_labels[:sample_size, -1:].ravel()\n",
    "for i in c_values:\n",
    "    clf = svm.LinearSVC(C=i)\n",
    "    clf.fit(train_data, train_labels)\n",
    "    val_acc = clf.score(val_data, val_labels)\n",
    "    collect_validation_acc.append(val_acc)\n",
    "    print(\"C =\", i, \": \", val_acc)\n",
    "maximum_c = collect_validation_acc.index(max(collect_validation_acc))\n",
    "\n",
    "print(\"The C value for maximum accuracy:{}\".format(c_values[maximum_c]))\n",
    "print()\n",
    "print(\"Accuracy of the maximum C:{}\".format(max(collect_validation_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do K-Fold Cross-Validation, I created a helper function called helper_kfold \n",
    "that would essentially deal with the partitioning and shuffling of the data.\n",
    "I essentially partitioned the data into \n",
    "[(0, 1034), (1034, 2068), (2068, 3103), (3103, 4137), (4137, 5172)] sets.\n",
    "Then I used the first set in order to validate each of the other sets and everytime\n",
    "it finishes the validation. It would restart and validate the other set.\n",
    "After that we follow what we did question 4, where the we attain the C values and\n",
    "it's accuracy respectively. At last I found that the C that gives the most\n",
    "optimal accuracy would be C = 16 and it's accuracy is about 0.81."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Problem 5a\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "spam = io.loadmat(\"data/spam_data.mat\")\n",
    "num_samples = len(spam[\"training_data\"])\n",
    "index_arr = np.random.choice(num_samples, num_samples, replace=False)\n",
    "\n",
    "def helper_kfold(sq, z):\n",
    "    average = len(sq) / int(z)\n",
    "    output = []\n",
    "    last = 0\n",
    "    while last < len(sq):\n",
    "        output.append((int(last), int(last+average)))\n",
    "        last = last + average\n",
    "    return output\n",
    "\n",
    "collect_validation_acc = []\n",
    "#c_values = list(range(1, 100, 10))\n",
    "c_values = [2**(i) for i in range(-5, 5, 1)]\n",
    "all_train_data = spam['training_data'][index_arr]\n",
    "all_train_labels = spam['training_labels'][index_arr].ravel()\n",
    "\n",
    "### Method of partitioning into 5 parts\n",
    "k = 5\n",
    "num = list(range(0, all_train_data.shape[0]))\n",
    "partition = helper_kfold(num, k)\n",
    "print(partition)\n",
    "\n",
    "for i in c_values:\n",
    "    k_acc = []\n",
    "    for j in np.arange(0, k, 1):\n",
    "        \n",
    "        # get paritions of data - valid and train\n",
    "        val_num = partition[j]\n",
    "        val_data = all_train_data[val_num[0]:val_num[1]]\n",
    "        val_labels = all_train_labels[val_num[0]:val_num[1]]        \n",
    "        t_data = np.concatenate([all_train_data[x:y] for x, y in partition if \n",
    "                                 (x, y) != val_num])\n",
    "        t_labels = np.concatenate([all_train_labels[x:y] for x, y in partition if \n",
    "                                   (x, y) != val_num])\n",
    "        \n",
    "        # clf functions\n",
    "        clf = svm.LinearSVC(C = i)\n",
    "        clf.fit(t_data, t_labels)\n",
    "        k_acc.append(clf.score(val_data, val_labels))\n",
    "    \n",
    "    print(\"C = {}\".format(i), \"Average Accuracy: {}\".format(np.mean(k_acc)))\n",
    "    collect_validation_acc.append(np.mean(k_acc))\n",
    "    \n",
    "max_num = collect_validation_acc.index(max(collect_validation_acc))\n",
    "print(\"The C that gives maximum accuracy: {}\".format(c_values[max_num]))\n",
    "print(\"Accuracy: {}\".format(max(collect_validation_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Kaggle - MNIST Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach was fairly simply, all I get around is about 95% accuracy by using kernel = \"linear\" and the optimal C value that was found in Question 4. Although this choice may have a lower accuracy compare to when kernel = \"poly\", but it seems to be running slower compare to the linear method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 6 Kaggle Mnist set\n",
    "np.random.seed(420)\n",
    "C = 1e-05\n",
    "mnist = io.loadmat(\"data/mnist_data.mat\")\n",
    "num_samples = len(mnist[\"training_data\"])\n",
    "index_arr = np.random.choice(num_samples, num_samples, replace=False)\n",
    "\n",
    "all_train_labels = mnist[\"training_labels\"][index_arr][10000:]\n",
    "all_train_data = mnist[\"training_data\"][index_arr][10000:]\n",
    "all_val_labels = mnist[\"training_labels\"][index_arr][:10000]\n",
    "all_val_data = mnist[\"training_data\"][index_arr][:10000]\n",
    "t_data = vstack([all_train_data, all_val_data])\n",
    "t_labels = np.append(all_train_labels.ravel(), all_val_labels.ravel())\n",
    "test = mnist[\"test_data\"]\n",
    "#clf = svm.SVC(C=C, kernel=\"poly\", degree=2)\n",
    "clf = svm.SVC(C=C, kernel=\"linear\")\n",
    "clf.fit(t_data, t_labels)\n",
    "print(clf.score(all_val_data, all_val_labels))\n",
    "t_predict = clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save, override\n",
    "save_csv.results_to_csv(t_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Kaggle - SPAM Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like problem 5, I believe that there isn't enough spam data to to work as a good training data. The way I approached this problem is by using question 5's optimal c value in order to predict the best accuracy rate. Once I found my optimal C value which is 8, then I used vstack to combine my data sets. Lastly, I've found that by using kernel = \"poly\" and degree = 2, would give me my best result which was about 78%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 6 Kaggle Spam set \n",
    "np.random.seed(421)\n",
    "C = 8\n",
    "spam = io.loadmat(\"data/spam_data.mat\")\n",
    "num_samples = len(spam[\"training_data\"])\n",
    "index_arr = np.random.choice(num_samples, num_samples, replace=False)\n",
    "\n",
    "spam_20 = int(spam[\"training_data\"][index_arr].shape[0] * 0.2)\n",
    "all_train_labels = spam[\"training_labels\"][index_arr][spam_20:]\n",
    "all_train_data = spam[\"training_data\"][index_arr][spam_20:]\n",
    "all_val_labels = spam[\"training_labels\"][index_arr][:spam_20]\n",
    "all_val_data = spam[\"training_data\"][index_arr][:spam_20]\n",
    "t_data = vstack([all_train_data, all_val_data])\n",
    "t_labels = np.append(all_train_labels.ravel(), all_val_labels.ravel())\n",
    "test = spam[\"test_data\"]\n",
    "clf = svm.SVC(C=C, kernel=\"poly\", degree=2)\n",
    "clf.fit(t_data, t_labels)\n",
    "print(clf.score(all_val_data, all_val_labels))\n",
    "t_predict = clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save, override\n",
    "save_csv.results_to_csv(t_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Kaggle - CIFAR-10 Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Cifar10 is a data set with natural images, which means that it would have a higher dimensional space. Therefore, it would make more sense that we choose using nonlinear classifier like polynomial kernel. When I initially tried using linear classifier and used a smaller train size where my num_train = 5000. The smaller train size gave me an indication that the C value is correct since my accuracy went up as I increase my train size. Also I was getting about 25% but when I use poly classifier, the accuracy rate became around 45% accuracy with the help of having a larger train size, which is num_train = 20000. Yet, I believe using poly kernel would result an inevitable slow run time, where it took around 15 mins to finish the code's execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 6 Kaggle cifar10 set\n",
    "np.random.seed(422)\n",
    "C = 0.1\n",
    "cifar10 = io.loadmat(\"data/cifar10_data.mat\")\n",
    "num_samples = len(cifar10[\"training_data\"])\n",
    "index_arr = np.random.choice(num_samples, num_samples, replace=False)\n",
    "\n",
    "test_data = cifar10[\"test_data\"]\n",
    "train_data = cifar10[\"training_data\"][index_arr]\n",
    "train_labels = cifar10[\"training_labels\"][index_arr].ravel()\n",
    "\n",
    "num_train = 20000\n",
    "t_data = train_data[:num_train]\n",
    "t_labels = train_labels[:num_train]\n",
    "val_data = train_data[num_train:num_train+1000]\n",
    "val_labels = train_labels[num_train:num_train+1000]\n",
    "print(\"Complete processing data\")\n",
    "\n",
    "clf = svm.SVC(C=C, kernel=\"poly\", degree=2, verbose = False, max_iter = 100000)\n",
    "#clf = svm.SVC(C=C, kernel=\"linear\", verbose = True, max_iter = 100000)\n",
    "clf.fit(t_data, t_labels)\n",
    "print(\"Training complete.\")\n",
    "print(\"Accuracy: {}\".format(clf.score(val_data, val_labels)))\n",
    "t_predict = clf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save, override\n",
    "save_csv.results_to_csv(t_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Complete processing data \n",
    "Training complete.\n",
    "Accuracy: 0.451\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gsExport\n",
    "gsExport.generateSubmission(\"HW1_submission--Henry Leou.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
